"""
Google Gemini APIë¥¼ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ê¸€ ìƒì„± ëª¨ë“ˆ
"""
import json
import re
import sys
import os
import hashlib
from datetime import datetime
from pathlib import Path

import google.generativeai as genai

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.settings import GEMINI_API_KEY
from src.templates.prompts import (
    EXPERIENCE_ARTICLE_PROMPT,
    UNIFIED_ARTICLE_PROMPT,
)


class ContentGenerator:
    """Gemini ê¸°ë°˜ ì½˜í…ì¸  ìƒì„±ê¸°"""

    # ìºì‹œ ë””ë ‰í† ë¦¬
    CACHE_DIR = Path(__file__).parent.parent / "data" / "cache"

    def __init__(self):
        if not GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        genai.configure(api_key=GEMINI_API_KEY)
        self.model = genai.GenerativeModel(
            "gemini-2.5-flash",
            generation_config={
                "response_mime_type": "application/json",
                "max_output_tokens": 8192,
            }
        )
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.CACHE_DIR.mkdir(parents=True, exist_ok=True)

    def _get_cache_key(self, news_titles: list, category: str) -> str:
        """ë‰´ìŠ¤ ì œëª© ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„± (ë‹¹ì¼ ê¸°ì¤€)"""
        today = datetime.now().strftime("%Y%m%d")
        titles_str = "|".join(sorted([t["title"] for t in news_titles[:10]]))
        content = f"{today}:{category}:{titles_str}"
        return hashlib.md5(content.encode()).hexdigest()[:12]

    def _get_cached_article(self, cache_key: str) -> dict | None:
        """ìºì‹œëœ ê¸€ ê°€ì ¸ì˜¤ê¸°"""
        cache_file = self.CACHE_DIR / f"{cache_key}.json"
        if cache_file.exists():
            with open(cache_file, "r", encoding="utf-8") as f:
                cached = json.load(f)
                # ë‹¹ì¼ ìºì‹œë§Œ ìœ íš¨
                if cached.get("cached_date") == datetime.now().strftime("%Y%m%d"):
                    print("ğŸ“¦ ìºì‹œëœ ê¸€ ì‚¬ìš©")
                    return cached.get("article")
        return None

    def _save_to_cache(self, cache_key: str, article: dict):
        """ê¸€ ìºì‹œì— ì €ì¥"""
        cache_file = self.CACHE_DIR / f"{cache_key}.json"
        cache_data = {
            "cached_date": datetime.now().strftime("%Y%m%d"),
            "cached_at": datetime.now().isoformat(),
            "article": article,
        }
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    def _parse_json_response(self, text: str) -> dict:
        """ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ"""
        original_text = text  # ë””ë²„ê¹…ìš© ì›ë³¸ ì €ì¥

        # JSON ë¸”ë¡ ì°¾ê¸°
        json_match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
        if json_match:
            text = json_match.group(1)

        # ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” JSON ì°¾ê¸°
        json_match = re.search(r"\{.*\}", text, re.DOTALL)
        if json_match:
            text = json_match.group(0)

        # ì²« ë²ˆì§¸ ì‹œë„: ê·¸ëŒ€ë¡œ íŒŒì‹±
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # ë‘ ë²ˆì§¸ ì‹œë„: content í•„ë“œ ë‚´ ë¬¸ì œê°€ ìˆëŠ” ë¬¸ì ìˆ˜ì •
        try:
            # JSON ë¬¸ìì—´ ë‚´ë¶€ì˜ ì´ìŠ¤ì¼€ì´í”„ ì•ˆ ëœ ì¤„ë°”ê¿ˆ ì²˜ë¦¬
            fixed_text = re.sub(r'(?<!\\)\n', '\\n', text)
            # ì´ìŠ¤ì¼€ì´í”„ ì•ˆ ëœ íƒ­ ì²˜ë¦¬
            fixed_text = re.sub(r'(?<!\\)\t', '\\t', fixed_text)
            return json.loads(fixed_text)
        except json.JSONDecodeError:
            pass

        # ì„¸ ë²ˆì§¸ ì‹œë„: í•„ë“œë³„ë¡œ ì¶”ì¶œí•´ì„œ ì¬êµ¬ì„±
        try:
            result = {}

            # ê¸°ë³¸ ë¬¸ìì—´ í•„ë“œë“¤ ì¶”ì¶œ
            string_fields = [
                "trend_summary", "reader_perspective", "selected_topic",
                "title", "meta_description", "category"
            ]
            for field in string_fields:
                match = re.search(rf'"{field}"\s*:\s*"([^"]*)"', text)
                if match:
                    result[field] = match.group(1)

            # content í•„ë“œ (HTML í¬í•¨, ë³µì¡í•¨) - tags ì•ê¹Œì§€ ì¶”ì¶œ
            content_match = re.search(r'"content"\s*:\s*"(.*?)",\s*"tags"', text, re.DOTALL)
            if content_match:
                content = content_match.group(1)
                # ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                content = content.replace('\n', '').replace('\r', '')
                result["content"] = content

            # tags ë°°ì—´ ì¶”ì¶œ
            tags_match = re.search(r'"tags"\s*:\s*\[(.*?)\]', text, re.DOTALL)
            if tags_match:
                tags_str = tags_match.group(1)
                tags = [t.strip().strip('"').strip("'") for t in tags_str.split(',') if t.strip()]
                result["tags"] = tags

            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if result.get("title") and result.get("content"):
                return result

        except Exception as e:
            print(f"í•„ë“œë³„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # ë„¤ ë²ˆì§¸ ì‹œë„: content í•„ë“œ ë‚´ ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì œ í•´ê²°
        try:
            # content ë‚´ë¶€ì˜ ì´ìŠ¤ì¼€ì´í”„ ì•ˆ ëœ ë”°ì˜´í‘œ ì²˜ë¦¬
            fixed_text = re.sub(r'(?<!\\)"(?=[^:,\[\]{}]*[,\]\}])', '\\"', text)
            return json.loads(fixed_text)
        except json.JSONDecodeError:
            pass

        raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨\nì›ë³¸: {original_text[:1000]}")

    def generate_experience_article(self, user_memo: str, category: str = "ì¼ìƒ/ë¦¬ë·°") -> dict:
        """ì²´í—˜í˜• ê¸€ ìƒì„±"""
        prompt = EXPERIENCE_ARTICLE_PROMPT.format(
            user_memo=user_memo,
            category=category,
        )

        response = self.model.generate_content(prompt)
        article = self._parse_json_response(response.text)

        article["article_type"] = "experience"
        article["user_memo"] = user_memo

        return article

    def generate_unified_article(self, news_data: dict, use_cache: bool = True) -> dict:
        """ë‰´ìŠ¤ íë¦„ ë¶„ì„ + ê¸€ ì‘ì„±ì„ 1íšŒ API í˜¸ì¶œë¡œ ì²˜ë¦¬

        Args:
            news_data: collect_news_titles()ì˜ ë°˜í™˜ê°’
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)

        Returns:
            ìƒì„±ëœ ë¸”ë¡œê·¸ ê¸€ ë°ì´í„°
        """
        category = news_data.get("category", "ai")
        category_name = news_data.get("category_name", "AI/í…Œí¬")
        titles = news_data.get("titles", [])

        # ìºì‹œ í™•ì¸
        if use_cache:
            cache_key = self._get_cache_key(titles, category)
            cached = self._get_cached_article(cache_key)
            if cached:
                return cached

        # ë‰´ìŠ¤ ì œëª©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        titles_str = "\n".join([f"- {item['title']}" for item in titles])

        # í†µí•© í”„ë¡¬í”„íŠ¸ë¡œ 1íšŒ API í˜¸ì¶œ
        print("ğŸ“° ë‰´ìŠ¤ ë¶„ì„ ë° ê¸€ ìƒì„± ì¤‘... (1íšŒ API í˜¸ì¶œ)")
        prompt = UNIFIED_ARTICLE_PROMPT.format(
            news_titles=titles_str,
            category_name=category_name,
        )

        response = self.model.generate_content(prompt)
        article = self._parse_json_response(response.text)

        # ê²°ê³¼ ì¶œë ¥
        print(f"ğŸ“Š íŠ¸ë Œë“œ: {article.get('trend_summary', '')}")
        print(f"ğŸ‘€ ë…ì ê´€ì : {article.get('reader_perspective', '')}")
        print(f"ğŸ’¡ ì„ ì •ëœ ì£¼ì œ: {article.get('selected_topic', '')}")
        print(f"âœï¸ ì œëª©: {article.get('title', '')}")

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        article["article_type"] = "unified"
        article["source_topic"] = article.get("selected_topic", "")

        # ìºì‹œ ì €ì¥
        if use_cache:
            self._save_to_cache(cache_key, article)
            print("ğŸ’¾ ìºì‹œì— ì €ì¥ë¨")

        return article


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    from src.news_collector import NewsCollector

    collector = NewsCollector()
    generator = ContentGenerator()

    print("=" * 50)
    print("í†µí•© ê¸€ ìƒì„± í…ŒìŠ¤íŠ¸ (1íšŒ API í˜¸ì¶œ)")
    print("=" * 50)

    # ë‰´ìŠ¤ ì œëª© ìˆ˜ì§‘
    news_data = collector.collect_news_titles()
    print(f"\nì¹´í…Œê³ ë¦¬: {news_data['category_name']}")
    print(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì œëª© ìˆ˜: {len(news_data['titles'])}")

    # í†µí•© ê¸€ ìƒì„±
    article = generator.generate_unified_article(news_data, use_cache=False)

    print(f"\n{'=' * 50}")
    print("=== ìƒì„±ëœ ê¸€ ===")
    print(f"{'=' * 50}")
    print(f"ì œëª©: {article['title']}")
    print(f"ë©”íƒ€: {article['meta_description']}")
    print(f"íƒœê·¸: {', '.join(article['tags'])}")
    print(f"\në³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°:\n{article['content'][:800]}...")
